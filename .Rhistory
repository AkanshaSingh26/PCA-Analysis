standardize <- function(x) {(x - mean(x))}
my.scaled.classes <- apply(final,2,standardize)
final_data <- dummy.data.frame(my_data, names = c("Item_Fat_Content","Item_Type",
"Outlet_Establishment_Year","Outlet_Size",
"Outlet_Location_Type","Outlet_Type"))
data1 <- read.csv("Train.csv", na.strings=c("","NA"))
data2 <- read.csv("Test.csv",na.strings=c("","NA"))
data2$Item_Outlet_Sales <- 1
new_data <- rbind(data1,data2)
new_data$Item_Weight[is.na(new_data$Item_Weight)] <- median(new_data$Item_Weight, na.rm = TRUE)
#Fat Content
levels(new_data$Item_Fat_Content)[1] <- "Low Fat"
levels(new_data$Item_Fat_Content)[2] <- "Low Fat"
levels(new_data$Item_Fat_Content)[2] <- "Regular"
#impute 0 with median
new_data$Item_Visibility <- ifelse(new_data$Item_Visibility == 0, median(new_data$Item_Visibility),new_data$Item_Visibility)
#find mode and impute
levels(new_data$Outlet_Size)[4] <- "Other"
new_data$Outlet_Size[is.na(new_data$Outlet_Size)] <- "Other"
#remove the dependent and identifier variables
my_data <- subset(new_data, select = -c(Item_Outlet_Sales, Item_Identifier,Outlet_Identifier))
final_data <- dummy.data.frame(my_data, names = c("Item_Fat_Content","Item_Type",
"Outlet_Establishment_Year","Outlet_Size",
"Outlet_Location_Type","Outlet_Type"))
colnames(final_data)
library(dummies)
final_data <- dummy.data.frame(my_data, names = c("Item_Fat_Content","Item_Type",
"Outlet_Establishment_Year","Outlet_Size",
"Outlet_Location_Type","Outlet_Type"))
colnames(final_data)
summary(final_data)
standardize <- function(x) {(x - mean(x))}
my.scaled.classes <- apply(final,2,standardize)
my.scaled.classes <- apply(final_data,2,standardize)
my.scaled.classes
norm_data <- scale(final_data, center = TRUE, scale = TRUE)
norm_data
str(norm_data)
library(dummies)
data1 <- read.csv("Train.csv", na.strings=c("","NA"))
data2 <- read.csv("Test.csv",na.strings=c("","NA"))
data2$Item_Outlet_Sales <- 1
new_data <- rbind(data1,data2)
new_data$Item_Weight[is.na(new_data$Item_Weight)] <- median(new_data$Item_Weight, na.rm = TRUE)
#Fat Content
levels(new_data$Item_Fat_Content)[1] <- "Low Fat"
levels(new_data$Item_Fat_Content)[2] <- "Low Fat"
levels(new_data$Item_Fat_Content)[2] <- "Regular"
#impute 0 with median
new_data$Item_Visibility <- ifelse(new_data$Item_Visibility == 0, median(new_data$Item_Visibility),new_data$Item_Visibility)
#find mode and impute
levels(new_data$Outlet_Size)[4] <- "Other"
new_data$Outlet_Size[is.na(new_data$Outlet_Size)] <- "Other"
#remove the dependent and identifier variables
my_data <- subset(new_data, select = -c(Item_Outlet_Sales, Item_Identifier,Outlet_Identifier))
final_data <- dummy.data.frame(my_data, names = c("Item_Fat_Content","Item_Type",
"Outlet_Establishment_Year","Outlet_Size",
"Outlet_Location_Type","Outlet_Type"))
prin_comp <- prcomp(final_data, scale. = T)
prin_comp
biplot(prin_comp, scale = 0)
data1$Item_Weight[is.na(data1$Item_Weight)] <- median(data1$Item_Weight, na.rm = TRUE)
data1 <- read.csv("Train.csv", na.strings=c("","NA"))
data1$Item_Weight[is.na(data1$Item_Weight)] <- median(data1$Item_Weight, na.rm = TRUE)
#Fat Content
levels(data1$Item_Fat_Content)[1] <- "Low Fat"
levels(data1$Item_Fat_Content)[2] <- "Low Fat"
levels(data1$Item_Fat_Content)[2] <- "Regular"
#impute 0 with median
data1$Item_Visibility <- ifelse(data1$Item_Visibility == 0, median(data1$Item_Visibility),data1$Item_Visibility)
#find mode and impute
levels(data1$Outlet_Size)[4] <- "Other"
data1$Outlet_Size[is.na(data1$Outlet_Size)] <- "Other"
#remove the dependent and identifier variables
my_data <- subset(data1, select = -c(Item_Outlet_Sales, Item_Identifier,Outlet_Identifier))
final_data <- dummy.data.frame(my_data, names = c("Item_Fat_Content","Item_Type",
"Outlet_Establishment_Year","Outlet_Size",
"Outlet_Location_Type","Outlet_Type"))
prin_comp <- prcomp(final_data, scale. = T)
biplot(prin_comp, scale = 0)
prin_comp
summary(prin_comp)
str(prin_comp)
sd(final_data[,1])
mean(final_data[,1])
apply(final_data[,1],function(x) {(x - mean(x))})
m <- mean(final_data[,1])
f <- final_data[,1] - m
f
sd(f)
sc <- f/sd(f)
sc
sd(sc)
standardize <- function(x) {(x - mean(x))}
centredData <- apply(final_data,2,standardize)
centredData
sd(centredData)
sd(centredData[,1:41])
normalize <- function(x) {(x/sd(x))}
scaledData <- apply(centredData,2,normalize)
scaledData
names(prin_comp)
prin_comp$center
mean(final_data[,1])
prin_comp$scale
sd(final_data[,1])
centredScaledData <- apply(centredData,2,normalize)
my.cov <- cov(centredScaledData)
my.eigen <- eigen(my.cov)
rownames(my.eigen$vectors) <- colnames(centredScaledData)
colnames(my.eigen$vectors) <- c("PC1","PC2","PC3","PC4","PC5","PC6","PC7","PC8","PC9","PC10",
"PC11","PC12","PC12","PC14","PC15","PC16","PC17","PC18","PC19","PC20",
"PC21","PC22","PC23","PC24","PC25","PC26","PC27","PC28","PC29","PC30",
"PC31","PC32","PC33","PC34","PC35","PC36","PC37","PC38","PC39","PC40","PC41"
)
loadings <- my.eigen$vectors
pc1.var <- 100*round(my.eigen$values[1]/sum(my.eigen$values),digits=3)
pc2.var <- 100*round(my.eigen$values[2]/sum(my.eigen$values),digits=3)
pc1.var
pc2.var
xlab=paste("PC1 - ",pc1.var," % of variation",sep="")
ylab=paste("PC2 - ",pc2.var," % of variation",sep="")
# transformaed data by multplyinh original data with eigen vector(PCA's)
scores <- centredScaledData %*% loadings
sd <- sqrt(my.eigen$values)
rownames(loadings) = colnames(centredScaledData)
plot(scores,ylim=c(-10,10),main="Data in terms of EigenVectors / PCs",xlab=,ylab=ylab)
abline(0,0,col="red")
abline(0,90,col="green")
scores.min <- min(scores[,1:2])
scores.max <- max(scores[,1:2])
plot(scores[,1]/sd[1],scores[,2]/sd[2],main="My First BiPlot",xlab=xlab,ylab=ylab,type="n")
rownames(scores)=seq(1:nrow(scores))
abline(0,0,col="red")
abline(0,90,col="green")
# This is to make the size of the lines more apparent
factor <- 5
# First plot the variables as vectors
arrows(0,0,loadings[,1]*sd[1]/factor,loadings[,2]*sd[2]/factor,length=0.1, lwd=2,angle=20, col="red")
text(loadings[,1]*sd[1]/factor*1.2,loadings[,2]*sd[2]/factor*1.2,rownames(loadings), col="red", cex=1.2)
# Second plot the scores as points
text(scores[,1]/sd[1],scores[,2]/sd[2], rownames(scores),col="blue", cex=0.7)
somelabs <- paste(round(centredScaledData[,1],digits=1),round(centredScaledData[,2],digits=1),sep=" , ")
plot(scores,ylim=c(-10,10),main="Data in terms of EigenVectors / PCs",xlab=xlab,ylab=ylab)
plot(scores,ylim=c(-3,3),main="Data in terms of EigenVectors / PCs",xlab=xlab,ylab=ylab)
abline(0,0,col="red")
abline(0,90,col="green")
scores.min <- min(scores[,1:2])
scores.max <- max(scores[,1:2])
plot(scores[,1]/sd[1],scores[,2]/sd[2],main="My First BiPlot",xlab=xlab,ylab=ylab,type="n")
rownames(scores)=seq(1:nrow(scores))
abline(0,0,col="red")
abline(0,90,col="green")
# This is to make the size of the lines more apparent
factor <- 2
# First plot the variables as vectors
arrows(0,0,loadings[,1]*sd[1]/factor,loadings[,2]*sd[2]/factor,length=0.1, lwd=2,angle=20, col="red")
text(loadings[,1]*sd[1]/factor*1.2,loadings[,2]*sd[2]/factor*1.2,rownames(loadings), col="red", cex=1.2)
# Second plot the scores as points
text(scores[,1]/sd[1],scores[,2]/sd[2], rownames(scores),col="blue", cex=0.7)
somelabs <- paste(round(centredScaledData[,1],digits=1),round(centredScaledData[,2],digits=1),sep=" , ")
my.eigen
my.eigen$values[1]
pc1.var <- 100*round(my.eigen$values[1]/sum(my.eigen$values),digits=2)
pc2.var <- 100*round(my.eigen$values[2]/sum(my.eigen$values),digits=3)
pc2.var <- 100*round(my.eigen$values[2]/sum(my.eigen$values),digits=2)
pc2.var <- 100*round(my.eigen$values[3]/sum(my.eigen$values),digits=2)
pc2.var <- 100*round(my.eigen$values[2]/sum(my.eigen$values),digits=2)
pc3.var <- 100*round(my.eigen$values[3]/sum(my.eigen$values),digits=2)
sum(my.eigen$values)
var(centredScaledData[,1])
standardize <- function(x) {(x - mean(x))}
centredData <- apply(final_data,2,standardize)
my.cov <- cov(centredData)
my.eigen <- eigen(my.cov)
rownames(my.eigen$vectors) <- colnames(centredData)
colnames(my.eigen$vectors) <- c("PC1","PC2","PC3","PC4","PC5","PC6","PC7","PC8","PC9","PC10",
"PC11","PC12","PC12","PC14","PC15","PC16","PC17","PC18","PC19","PC20",
"PC21","PC22","PC23","PC24","PC25","PC26","PC27","PC28","PC29","PC30",
"PC31","PC32","PC33","PC34","PC35","PC36","PC37","PC38","PC39","PC40","PC41"
)
sum(my.eigen$values)
var(centredData[,1])
centredData[,1]
library(dummies)
data1 <- read.csv("Train.csv", na.strings=c("","NA"))
data1$Item_Weight[is.na(data1$Item_Weight)] <- median(data1$Item_Weight, na.rm = TRUE)
#Fat Content
levels(data1$Item_Fat_Content)[1] <- "Low Fat"
levels(data1$Item_Fat_Content)[2] <- "Low Fat"
levels(data1$Item_Fat_Content)[2] <- "Regular"
#impute 0 with median
data1$Item_Visibility <- ifelse(data1$Item_Visibility == 0, median(data1$Item_Visibility),data1$Item_Visibility)
#find mode and impute
levels(data1$Outlet_Size)[4] <- "Other"
data1$Outlet_Size[is.na(data1$Outlet_Size)] <- "Other"
#remove the dependent and identifier variables
my_data <- subset(data1, select = -c(Item_Outlet_Sales, Item_Identifier,Outlet_Identifier))
final_data <- dummy.data.frame(my_data, names = c("Item_Fat_Content","Item_Type",
"Outlet_Establishment_Year","Outlet_Size",
"Outlet_Location_Type","Outlet_Type"))
colnames(final_data)
summary(final_data)
prin_comp <- prcomp(final_data, scale. = T)
biplot(prin_comp, scale = 0)
summary(prin_comp)
standardize <- function(x) {(x - mean(x))}
centredData <- apply(final_data,2,standardize)
my.cov <- cov(centredData)
my.eigen <- eigen(my.cov)
rownames(my.eigen$vectors) <- colnames(centredData)
colnames(my.eigen$vectors) <- c("PC1","PC2","PC3","PC4","PC5","PC6","PC7","PC8","PC9","PC10",
"PC11","PC12","PC12","PC14","PC15","PC16","PC17","PC18","PC19","PC20",
"PC21","PC22","PC23","PC24","PC25","PC26","PC27","PC28","PC29","PC30",
"PC31","PC32","PC33","PC34","PC35","PC36","PC37","PC38","PC39","PC40","PC41"
)
sum(my.eigen$values)
sum(var(centredData[,1:41]))
loadings <- my.eigen$vectors
pc1.var <- 100*round(my.eigen$values[1]/sum(my.eigen$values),digits=2)
pc2.var <- 100*round(my.eigen$values[2]/sum(my.eigen$values),digits=2)
pc3.var <- 100*round(my.eigen$values[3]/sum(my.eigen$values),digits=2)
xlab=paste("PC1 - ",pc1.var," % of variation",sep="")
ylab=paste("PC2 - ",pc2.var," % of variation",sep="")
# transformaed data by multplyinh original data with eigen vector(PCA's)
scores <- centredData %*% loadings
sd <- sqrt(my.eigen$values)
rownames(loadings) = colnames(centredData)
plot(scores,ylim=c(-3,3),main="Data in terms of EigenVectors / PCs",xlab=xlab,ylab=ylab)
abline(0,0,col="red")
abline(0,90,col="green")
scores.min <- min(scores[,1:2])
scores.max <- max(scores[,1:2])
plot(scores[,1]/sd[1],scores[,2]/sd[2],main="My First BiPlot",xlab=xlab,ylab=ylab,type="n")
rownames(scores)=seq(1:nrow(scores))
abline(0,0,col="red")
abline(0,90,col="green")
# This is to make the size of the lines more apparent
factor <- 2
# First plot the variables as vectors
arrows(0,0,loadings[,1]*sd[1]/factor,loadings[,2]*sd[2]/factor,length=0.1, lwd=2,angle=20, col="red")
text(loadings[,1]*sd[1]/factor*1.2,loadings[,2]*sd[2]/factor*1.2,rownames(loadings), col="red", cex=1.2)
# Second plot the scores as points
text(scores[,1]/sd[1],scores[,2]/sd[2], rownames(scores),col="blue", cex=0.7)
somelabs <- paste(round(centredData[,1],digits=1),round(centredData[,2],digits=1),sep=" , ")
data1 <- read.csv("Train.csv", na.strings=c("","NA"))
library(dummies)
data1 <- read.csv("Train.csv", na.strings=c("","NA"))
data1$Item_Weight[is.na(data1$Item_Weight)] <- median(data1$Item_Weight, na.rm = TRUE)
#Fat Content
levels(data1$Item_Fat_Content)[1] <- "Low Fat"
levels(data1$Item_Fat_Content)[2] <- "Low Fat"
levels(data1$Item_Fat_Content)[2] <- "Regular"
#impute 0 with median
data1$Item_Visibility <- ifelse(data1$Item_Visibility == 0, median(data1$Item_Visibility),data1$Item_Visibility)
#find mode and impute
levels(data1$Outlet_Size)[4] <- "Other"
data1$Outlet_Size[is.na(data1$Outlet_Size)] <- "Other"
#remove the dependent and identifier variables
my_data <- subset(data1, select = -c(Item_Outlet_Sales, Item_Identifier,Outlet_Identifier))
final_data <- dummy.data.frame(my_data, names = c("Item_Fat_Content","Item_Type",
"Outlet_Establishment_Year","Outlet_Size",
"Outlet_Location_Type","Outlet_Type"))
colnames(final_data)
summary(final_data)
standardize <- function(x) {(x - mean(x))}
centredData <- apply(final_data,2,standardize)
my.cov <- cov(centredData)
my.eigen <- eigen(my.cov)
rownames(my.eigen$vectors) <- colnames(centredData)
colnames(my.eigen$vectors) <- c("PC1","PC2","PC3","PC4","PC5","PC6","PC7","PC8","PC9","PC10",
"PC11","PC12","PC12","PC14","PC15","PC16","PC17","PC18","PC19","PC20",
"PC21","PC22","PC23","PC24","PC25","PC26","PC27","PC28","PC29","PC30",
"PC31","PC32","PC33","PC34","PC35","PC36","PC37","PC38","PC39","PC40","PC41"
)
sum(my.eigen$values)
sum(var(centredData[,1:41]))
loadings <- my.eigen$vectors
pc1.var <- 100*round(my.eigen$values[1]/sum(my.eigen$values),digits=3)
pc2.var <- 100*round(my.eigen$values[2]/sum(my.eigen$values),digits=3)
xlab=paste("PC1 - ",pc1.var," % of variation",sep="")
ylab=paste("PC2 - ",pc2.var," % of variation",sep="")
# transformaed data by multplyinh original data with eigen vector(PCA's)
scores <- centredData %*% loadings
sd <- sqrt(my.eigen$values)
rownames(loadings) = colnames(centredData)
plot(scores,ylim=c(-3,3),main="Data in terms of EigenVectors / PCs",xlab=xlab,ylab=ylab)
abline(0,0,col="red")
abline(0,90,col="green")
scores.min <- min(scores[,1:2])
scores.max <- max(scores[,1:2])
plot(scores[,1]/sd[1],scores[,2]/sd[2],main="My First BiPlot",xlab=xlab,ylab=ylab,type="n")
rownames(scores)=seq(1:nrow(scores))
abline(0,0,col="red")
abline(0,90,col="green")
# This is to make the size of the lines more apparent
factor <- 2
# First plot the variables as vectors
arrows(0,0,loadings[,1]*sd[1]/factor,loadings[,2]*sd[2]/factor,length=0.1, lwd=2,angle=20, col="red")
# This is to make the size of the lines more apparent
factor <- 1
# First plot the variables as vectors
arrows(0,0,loadings[,1]*sd[1]/factor,loadings[,2]*sd[2]/factor,length=0.1, lwd=2,angle=20, col="red")
plot(scores[,1]/sd[1],scores[,2]/sd[2],main="My First BiPlot",xlab=xlab,ylab=ylab,type="n")
rownames(scores)=seq(1:nrow(scores))
abline(0,0,col="red")
abline(0,90,col="green")
# This is to make the size of the lines more apparent
factor <- 1
# First plot the variables as vectors
arrows(0,0,loadings[,1]*sd[1]/factor,loadings[,2]*sd[2]/factor,length=0.1, lwd=2,angle=20, col="red")
text(loadings[,1]*sd[1]/factor*1.2,loadings[,2]*sd[2]/factor*1.2,rownames(loadings), col="red", cex=1.2)
rownames(scores)=seq(1:nrow(scores))
plot(scores[,1]/sd[1],scores[,2]/sd[2],main="My First BiPlot",xlab=xlab,ylab=ylab,type="n")
rownames(scores)=seq(1:nrow(scores))
abline(0,0,col="red")
abline(0,90,col="green")
# This is to make the size of the lines more apparent
factor <- .1
# First plot the variables as vectors
arrows(0,0,loadings[,1]*sd[1]/factor,loadings[,2]*sd[2]/factor,length=0.1, lwd=2,angle=20, col="red")
plot(scores[,1]/sd[1],scores[,2]/sd[2],main="My First BiPlot",xlab=xlab,ylab=ylab,type="n")
rownames(scores)=seq(1:nrow(scores))
abline(0,0,col="red")
abline(0,90,col="green")
# This is to make the size of the lines more apparent
factor <- .001
# First plot the variables as vectors
arrows(0,0,loadings[,1]*sd[1]/factor,loadings[,2]*sd[2]/factor,length=0.1, lwd=2,angle=20, col="red")
plot(scores[,1]/sd[1],scores[,2]/sd[2],main="My First BiPlot",xlab=xlab,ylab=ylab,type="n")
rownames(scores)=seq(1:nrow(scores))
abline(0,0,col="red")
abline(0,90,col="green")
# This is to make the size of the lines more apparent
factor <- .01
# First plot the variables as vectors
arrows(0,0,loadings[,1]*sd[1]/factor,loadings[,2]*sd[2]/factor,length=0.1, lwd=2,angle=20, col="red")
text(loadings[,1]*sd[1]/factor*1.2,loadings[,2]*sd[2]/factor*1.2,rownames(loadings), col="red", cex=1.2)
# Second plot the scores as points
text(scores[,1]/sd[1],scores[,2]/sd[2], rownames(scores),col="blue", cex=0.7)
somelabs <- paste(round(centredData[,1],digits=1),round(centredData[,2],digits=1),sep=" , ")
var(centredData[,1]) + var(my.scaled.classes[,2])+ var(my.scaled.classes[,3]) + var(my.scaled.classes[,4]) +
var(my.scaled.classes[,5]) + var(my.scaled.classes[,6]) + var(my.scaled.classes[,7]) + var(my.scaled.classes[,8]) +
var(my.scaled.classes[,9]) + var(my.scaled.classes[,10]) + var(my.scaled.classes[,11]) + var(my.scaled.classes[,12]) +
var(my.scaled.classes[,13]) + var(my.scaled.classes[,14]) + var(my.scaled.classes[,15]) + var(my.scaled.classes[,16]) +
var(my.scaled.classes[,17]) + var(my.scaled.classes[,18]) + var(my.scaled.classes[,19]) + var(my.scaled.classes[,20]) +
var(my.scaled.classes[,21]) + var(my.scaled.classes[,22]) + var(my.scaled.classes[,23]) + var(my.scaled.classes[,24])+
var(my.scaled.classes[,25]) + var(my.scaled.classes[,26]) + var(my.scaled.classes[,27]) + var(my.scaled.classes[,28]) +
var(my.scaled.classes[,29]) + var(my.scaled.classes[,30]) + var(my.scaled.classes[,31]) + var(my.scaled.classes[,32]) +
var(my.scaled.classes[,33]) + var(my.scaled.classes[,34]) + var(my.scaled.classes[,35]) + var(my.scaled.classes[,36]) +
var(my.scaled.classes[,37]) + var(my.scaled.classes[,38]) + var(my.scaled.classes[,39]) + var(my.scaled.classes[,40]) + var(my.scaled.classes[,41])
# Note that the sum of the eigen values equals the total variance of the data
sum(my.eigen$values)
sd
my.eigen$values
plot(scores[,1]/sd[1],scores[,2]/sd[2],main="My First BiPlot",xlab=xlab,ylab=ylab,type="n")
rownames(scores)=seq(1:nrow(scores))
abline(0,0,col="red")
abline(0,90,col="green")
# This is to make the size of the lines more visible
factor <- .01
# First plot the variables as vectors
arrows(0,0,loadings[,1]*sd[1]/factor,loadings[,2]*sd[2]/factor,length=0.1, lwd=2,angle=20, col="red")
text(loadings[,1]*sd[1]/factor*1.2,loadings[,2]*sd[2]/factor*1.2,rownames(loadings), col="red", cex=1.2)
# Second plot the scores as points
text(scores[,1]/sd[1],scores[,2]/sd[2], rownames(scores),col="blue", cex=0.7)
plot(scores[,1]/sd[1],scores[,2]/sd[2],main="My First BiPlot",xlab=xlab,ylab=ylab,type="n")
rownames(scores)=seq(1:nrow(scores))
rownames(scores)
1:nrow(scores
abline(0,0,col="red")
plot(scores[,1]/sd[1],scores[,2]/sd[2],main="My First BiPlot",xlab=xlab,ylab=ylab,type="n")
abline(0,0,col="red")
abline(0,90,col="green")
# This is to make the size of the lines more visible
factor <- .01
# First plot the variables as vectors
arrows(0,0,loadings[,1]*sd[1]/factor,loadings[,2]*sd[2]/factor,length=0.1, lwd=2,angle=20, col="red")
text(loadings[,1]*sd[1]/factor*1.2,loadings[,2]*sd[2]/factor*1.2,rownames(loadings), col="red", cex=1.2)
somelabs <- paste(round(centredData[,1],digits=1),round(centredData[,2],digits=1),sep=" , ")
# Second plot the scores as points
text(scores[,1]/sd[1],scores[,2]/sd[2], rownames(scores),col="black")
plot(scores[,1]/sd[1],scores[,2]/sd[2],main="My First BiPlot",xlab=xlab,ylab=ylab,type="n")
rownames(scores)=seq(1:nrow(scores))
abline(0,0,col="red")
abline(0,90,col="green")
# This is to make the size of the lines more visible
factor <- .01
# First plot the variables as vectors
arrows(0,0,loadings[,1]*sd[1]/factor,loadings[,2]*sd[2]/factor,length=0.1, lwd=2,angle=20, col="red")
text(loadings[,1]*sd[1]/factor*1.2,loadings[,2]*sd[2]/factor*1.2,rownames(loadings), col="red", cex=1.2)
# Second plot the scores as points
points(scores[,1]/sd[1],scores[,2]/sd[2], rownames(scores),col="black", cex=0.7)
# Second plot the scores as points
dotchart(scores[,1]/sd[1],scores[,2]/sd[2], rownames(scores),col="black", cex=0.7)
plot(scores[,1]/sd[1],scores[,2]/sd[2],main="My First BiPlot",xlab=xlab,ylab=ylab,type="n")
rownames(scores)=seq(1:nrow(scores))
abline(0,0,col="red")
abline(0,90,col="green")
# This is to make the size of the lines more visible
factor <- .01
# First plot the variables as vectors
arrows(0,0,loadings[,1]*sd[1]/factor,loadings[,2]*sd[2]/factor,length=0.1, lwd=2,angle=20, col="red")
text(loadings[,1]*sd[1]/factor*1.2,loadings[,2]*sd[2]/factor*1.2,rownames(loadings), col="red", cex=1.2)
# Second plot the scores as points
text(scores[,1]/sd[1],scores[,2]/sd[2], rownames(scores),col="black", cex=0.7)
somelabs
identify(scores[,1]/sd[1],scores[,2]/sd[2],labels=somelabs,cex=0.8)
